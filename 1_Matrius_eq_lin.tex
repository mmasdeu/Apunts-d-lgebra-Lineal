% !TeX encoding = UTF-8
% !TeX spellcheck = ca_ES-valencia
% !TeX root = MatCADAlgLin.tex
El contingut d'aquesta secció el podem trobar a \cite[Tema 1]{Bret} i a \cite[Tema 2]{NaXa}.
\begin{definicio}
	Si $m$ i $n$ són dos nombres naturals, una \emph{matriu $m\times n$ amb entrades a $\K$}\index{matriu} és una taula rectangular d'elements de $\K$ amb $m$ files i $n$ columnes. Denotem $M_{m\times n}(\K)$ al conjunt de matrius que tenen $m$ files i $n$ columnes i els seus elements són de $\K$.
\end{definicio}
\begin{notacio}
	Denotarem amb lletres majúscules el nom de les matrius i amb la mateixa lletra i subíndexs cadascun dels coeficients: si $A$ és una matriu, anomenarem $a_{ij}$ al nombre de la fila $i$, columna $j$. En el producte de matrius, a vegades utilitzarem la notació $(AB)_{ij}$ per a fer referència al coeficient de la posició $(i,j)$ després de fer el producte.
\end{notacio}
\begin{exemple}
	Si $A=\big(\begin{smallmatrix}
	1 & 3 & 0 \\ 0 & -1 & 1
	\end{smallmatrix}\big) \in M_{2\times3}(\Q)$, llavors $a_{11}=1$, $a_{12}=3$, $a_{13}=0$, $a_{21}=0$, $a_{22}=-1$ i $a_{23}=1$.
\end{exemple}
A continuació fixem algunes notacions i definicions de casos particulars:
\begin{itemize}
	\item Una \emph{matriu quadrada}\index{matriu!quadrada} és una matriu amb el nombre de columnes igual al nombre de files. Denotarem per $M_n(\K)=M_{n\times n}(\K)$.
	\item Un element està a la \emph{diagonal d'una matriu quadrada}\index{element!diagonal} si la posició que ocupa té el mateix nombre de fila que de columna: si la matriu és $A$, els elements de la diagonal són els $a_{ii}$.  
	\item Una \emph{matriu diagonal}\index{matriu!diagonal} és una matriu quadrada on els únics elements no nuls estan a la diagonal: $A$, matriu quadrada, és diagonal si $a_{ij}=0$ $\forall i\neq j$. 
	\item La \emph{matriu identitat $n\times n$}\index{matriu!identitat} és una matriu diagonal on tots els elements de la diagonal valen $1$ (i per tant els altres valen $0$). La denotem per $\1_n$ la matriu identitat $n\times n$.
	\item En general, escriurem els vectors\index{vector} per columnes: un \emph{vector de $\K^n$} és una matriu amb $n$ files i $1$ columna.
	\item Direm que una matriu quadrada $A$ és \emph{triangular superior}\index{matriu!triangular inferior} si tots els coeficients per sota de la diagonal valen $0$, o sigui, $a_ {ij}=0$ si $i>j$.
	\item Direm que una matriu quadrada $A$ és \emph{triangular inferior}\index{matriu!triangular superior} si tots els coeficients per sobre de la diagonal valen $0$, o sigui, $a_ {ij}=0$ si $i<j$.
	\item Donada una matriu $A \in M_{m\times n}(\K)$, definim la \emph{transposada d'$A$}\index{matriu!transposada} i la denotem per $A^T$ com la matriu de $M_{n\times m}(\K)$ que té per columnes les files d'$A$, o sigui, que a la posició $(i,j)$ té el coeficient $a_{ji}$. Tenim la propietat:
	$$
	(A^T)^T=A \,.
	$$
	\item Diem que una matriu $A$ és \emph{simètrica}\index{matriu!simètrica} si $A=A^T$ (en particular, ha de ser quadrada). Per exemple, la matriu identitat $\1_n$ és simètrica.
\end{itemize}

\begin{exemple}\index{sistema d'equacions}
	Considerem un sistema d'equacions amb $m$ equacions i $n$ incògnites:
	\begin{align*}
	a_{11}x_1+a_{12}x_2+ \cdots + a_{1n}x_n &= b_1 \\
	a_{21}x_1+a_{22}x_2+ \cdots + a_{2n}x_n &= b_2 \\
	&\vdots \\
	a_{m1}x_1+a_{m2}x_2+ \cdots + a_{mn}x_n &= b_m
	\end{align*}
	
	D'aquí podem treure la matriu associada\index{sistema d'equacions!matriu associada} als coeficients del sistema:
	\[A=
	\begin{pmatrix*}[l]
	a_{11} & a_{12} & \cdots & a_{1n} \\
	a_{21} & a_{22} & \cdots & a_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{m1} & a_{m2} & \cdots & a_{mn} 
	\end{pmatrix*}
	\]
	el vector de termes independents\index{sistema d'equacions!termes independents}:
	\[
	B=
	\begin{pmatrix}
	b_1 \\ b_2 \\ \vdots \\ b_m
	\end{pmatrix}
	\]
	o bé escriure-ho tot en una sola matriu (matriu ampliada)\index{sistema d'equacions!matriu ampliada}, on habitualment separem els termes independents:
	\[
	\begin{amatrix}{4}
	a_{11} & a_{12} & \cdots & a_{1n} & b_1 \\
	a_{21} & a_{22} & \cdots & a_{2n} & b_2 \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	a_{m1} & a_{m2} & \cdots & a_{mn} & b_m 
	\end{amatrix}
	\]
\end{exemple}
\subsection{Operacions amb matrius. Matriu invertible}\index{matriu!operacions}\label{subsec:opmat}
Considerem $\lambda \in \K$ i $A,B \in M_{m\times n}(\K)$. Les primeres operacions que podem fer són les que corresponen a veure $M_{m\times n} (\K)$ com $\K$-espai vectorial (més endavant veurem què vol dir):
\begin{itemize}
	\item Definim la matriu $\lambda A$ com la matriu que té per coeficients $\lambda a_{ij}$.\index{matriu!producte per escalar}
	\item Definim la matriu $A+B$ com la matriu que té per coeficients $a_{ij}+b_{ij}$.\index{matriu!suma}
\end{itemize}
\begin{exemple}
	\[
	\begin{pmatrix*}[r] -1 & 1 & 0 \\ 1 & 2 & 1 \end{pmatrix*} +
	\begin{pmatrix*}[r] 0 & -1 & 2 \\ -1 & 3 & 2 \end{pmatrix*} =
	\begin{pmatrix*}[r] -1 & 0 & 2 \\ 0 & 5 & 3 \end{pmatrix*}
	\]
\end{exemple}
\begin{exemple}
	$$
	2 \begin{pmatrix} -1 & 1 & 0 \\ 1 & 2 & 1 \end{pmatrix} =
	\begin{pmatrix} -2 & 2 & 0 \\ 2 & 4 & 2 \end{pmatrix} 
	$$
\end{exemple}
Aquestes definicions, més les propietats dels elements de $\K$ impliquen:
\begin{enumerate}
	\item Existeix una matriu $\0_{mn}$ complint $A+\0_{mn}=A$, $\forall A \in M_{mn}(\K)$ ($\0_{mn}$ té tots els coeficients zero).
	\item $0 A = \0_{mn}$, $\forall A \in M_{m\times n}(\K)$.
	\item $A+B=B+A$, $\forall A,B \in M_{m\times n}(\K)$.
	\item $1A=A$, $\forall A \in M_{m\times n}(\K)$.
	\item $\lambda (\mu A)= (\lambda \mu) A$, $\forall \lambda,\mu \in \K$ i $\forall A \in M_{m\times n}(\K)$
\end{enumerate}

Abans de la definició del producte de matrius és convenient utilitzar el llenguatge següent:
\begin{definicio}\label{def:dependlineal}
	Diem que \emph{un vector $\vec{w}$ és combinació lineal de $\{\vec{v}_1,\dots,\vec{v}_n\}$}\index{vector!combinació lineal} si existeixen escalars $\lambda_1, \dots , \lambda_n$ tals que $w=\lambda_1\vec{v}_1+\cdots+\lambda_n\vec{v}_n$.
	
	Si un dels vectors de $\{\vec{v}_1,\dots,\vec{v}_n\}$ es pot escriure com a combinació lineal dels altres, diem que \emph{la família de vectors $\{\vec{v}_1,\dots,\vec{v}_n\}$ és linealment dependent}\index{vector!dependència lineal}.
	
	En cas contrari, diem que \emph{la família de vectors $\{\vec{v}_1,\dots,\vec{v}_n\}$ és linealment independent}\index{vector!independència lineal}.
\end{definicio}

Parlarem de files (o columnes) linealment dependents o independents d'una matriu $A$ pensades com a vectors amb tantes components com files (o columnes) tingui $A$.

\begin{exercici}
	Demostreu que $\{\vec{v}_1,\dots,\vec{v}_n\}$ són linealment independents si l'única manera d'escriure el vector zero $\vec{0}=\lambda_1\vec{v}_1+\cdots+\lambda_n\vec{v}_n$ és amb $\lambda_1=\cdots\lambda_n=0$.
\end{exercici}

A més, a més, podem definir:
\begin{definicio}\index{matriu!producte de matrius}
	Si $A \in M_{m\times n}(\K)$, $B \in M_{n\times r}(\K)$ (o sigui, el nombre de columnes de $A$ és igual al nombre de files de $B$) podem definir el \emph{producte $AB$} com la matriu $C\in M_{m\times r}$ que té per coeficients:
	\[
	c_{ij}=\sum_{k=1}^{n} a_{ik}b_{kj} \,.
	\]
	La matriu $C$ és pot pensar que té per columnes combinacions lineals de columnes de $A$ ($B$ ens diu quines són aquestes combinacions lineals): la columna $j$ de la matriu $C$ és:
	\[
	\begin{pmatrix}
	c_{1j}\\c_{2j}\\ \vdots \\ c_{mj} 
	\end{pmatrix} =
	b_{1j}	\begin{pmatrix}
	a_{11}\\a_{21}\\ \vdots \\ a_{m1} 
	\end{pmatrix} +
	b_{2j}	\begin{pmatrix}
	a_{12}\\a_{22}\\ \vdots \\ a_{m2} 
	\end{pmatrix} + \cdots +
	b_{nj}	\begin{pmatrix}
	a_{1n}\\a_{2n}\\ \vdots \\ a_{mn} 
	\end{pmatrix}
	\]
	Anàlogament, la matriu $C$ es pot pensar que té per files combinacions lineals de files de $B$ ($A$ ens diu quines són aquestes combinacions lineals): la fila $i$ de la matriu $C$ és (separem amb comes per a que quedi més clar):
	\[
	(c_{i1},c_{i2},\cdots,c_{ir})=
	a_{i1} (b_{11},b_{12},\dots,b_{1r})+
	a_{i2} (b_{21},b_{22},\dots,b_{2r})+ \cdots +
	a_{in} (b_{n1},b_{n2},\dots,b_{nr})
	\]
\end{definicio}
\begin{exemple}\label{exempl:prodmat}
	$$
	\begin{pmatrix*}[r] -1 & 1 & 0 \\ 1 & 2 & 1 \end{pmatrix*}
	\begin{pmatrix*}[r] 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{pmatrix*}
	= 	\begin{pmatrix*}[r] 2 & 2 \\ 12 & 16 \end{pmatrix*}
	$$
	$$
	\begin{pmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{pmatrix}
	\begin{pmatrix} -1 & 1 & 0 \\ 1 & 2 & 1 \end{pmatrix}
	= 	\begin{pmatrix} 1 & 5 & 2 \\ 1 & 11 & 4 \\ 1 & 17 & 6 \end{pmatrix}
	$$
\end{exemple}

\begin{exemple}\index{sistema d'equacions}
	Podem escriure el sistema d'equacions
	\begin{align*}
	a_{11}x_1+a_{12}x_2+ \cdots + a_{1n}x_n &= b_1 \\
	a_{21}x_1+a_{22}x_2+ \cdots + a_{2n}x_n &= b_2 \\
	&\vdots \\
	a_{m1}x_1+a_{m2}x_2+ \cdots + a_{mn}x_n &= b_m
	\end{align*}
	com $AX=B$, on
	\[
	A=
	\begin{pmatrix}
	a_{11} & a_{12} & \cdots & a_{1n} \\
	a_{21} & a_{22} & \cdots & a_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{m1} & a_{m2} & \cdots & a_{mn} 
	\end{pmatrix},\quad
	X=
	\begin{pmatrix}
	x_1 \\ x_2 \\ \vdots \\ x_n
	\end{pmatrix} \text{ i}\quad
	B=
	\begin{pmatrix}
	b_1 \\ b_2 \\ \vdots \\ b_m
	\end{pmatrix}.
	\]
\end{exemple}

\begin{proposicio}
	El producte de matrius té les propietats següents:
	\begin{enumerate}[\rm (a)]
		\item Element neutre: si $A \in M_{m\times n}(\K)$, $\1_m A = A \1_n = A$.
		\item Propietat associativa: si $A \in M_{m\times n}(\K)$, $B \in M_{n\times r}(\K)$, $C \in M_{r\times s}(\K)$, llavors
		\[(AB)C=A(BC).\]
		\item Distributiva respecte el producte: si $A \in M_{m\times n}(\K)$, $B, C \in M_{n\times r}(\K)$ i $D \in M_{r\times s}(\K)$, llavors $A(B+C)=AB+AC$ i $(B+C)D=BD+CD$.
	\end{enumerate}
\end{proposicio}
\begin{proof}
	Escriure les fórmules amb els coeficients i surt. Fem la propietat associativa com exemple: volem comparar el coeficient a la posició $(i,j)$ d'$(AB)C$ amb el d'$A(BC)$, que denotem $((AB)C)_{ij}$ i $(A(BC))_{ij}$ respectivament:
	$$
	((AB)C)_{ij}=\sum_{k=1}^r(AB)_{ik}c_{kj}=\sum_{k=1}^r(\sum_{l=1}^na_{il}b_{lk})c_{kj}=\sum_{k=1}^r\sum_{l=1}^na_{il}b_{lk}c_{kj}
	$$
	Mentre que:
	$$
	(A(BC))_{ij}=\sum_{l=1}^na_{il}(BC)_{lj}=\sum_{l=1}^na_{il}(\sum_{k=1}^rb_{lk}c_{kj})=\sum_{l=1}^n\sum_{k=1}^ra_{il}b_{lk}c_{kj}
	$$
	I els dos resultats són el mateix ja que podem commutar els sumatoris.
\end{proof}
\begin{observacio}
	El producte de matrius, en general, no és commutatiu (veure l'Exemple \ref{exempl:prodmat}).
\end{observacio}
\begin{proposicio}
	Si $A\in M_{m\times n}(\K)$ i $B\in M_{n\times r}(\K)$, llavors tenim la relació següent entre productes i transposades
	$$
	(AB)^T=B^T A^T
	$$
\end{proposicio}
\begin{proof}
	Escrivim les fórmules dels coeficients:
	$$
	((AB)^T)_{ij}=(AB)_{ji}=\sum_{k=1}^n a_{jk}b_{ki}
	$$
	mentre que
	$$
	(B^TA^T)_{ij}=\sum_{k=1}^n (B^T)_{ik}(A^T)_{kj}=\sum_{k=1}^n b_{ki}a_{jk}
	$$
	i són iguals per la propietat commutativa del producte a $\K$.
\end{proof}
\begin{definicio}\index{producte escalar}
	Si considerem $\vec{v}$ i $\vec{w}$ vectors de $\K^n$, que escrivim com una columna cadascun, definim el \emph{producte escalar $\vec{v}\cdot\vec{w}$} com:
	$$
	\vec{v}\cdot \vec{w}=\vec{v}^T \vec{w}
	$$
	Per tant, si les coordenades són:
	$$
	\vec{v}=\begin{pmatrix}
	v_1 \\ v_2 \\ \vdots \\ v_n 
	\end{pmatrix}, 
	\vec{w}=\begin{pmatrix}
	w_1 \\ w_2 \\ \vdots \\ w_n 
	\end{pmatrix}
	\text{ llavors }
	\vec{v}\cdot\vec{w}=\sum_{i=1}^n v_iw_i \,.
	$$
\end{definicio}

Volem definir la inversa d'una matriu quadrada, però com que el producte no és commutatiu, hauríem de parlar d'inversa per l'esquerra o per la dreta.
\begin{definicio}\index{matriu!inversa}
	Diem que una \emph{matriu quadrada $A \in M_{n}(\K)$ és invertible} si existeix una matriu $B \in M_n(\K)$ tal que $AB=BA=\1_n$.\\
	%Diem que una \emph{matriu quadrada $A \in M_{n}(\K)$ és invertible per l'esquerra} si existeix una matriu $C \in M_n(\K)$ tal que $CA=\1_n$.
\end{definicio}
\begin{teorema}\label{teo:invuniq}
	Si $A$ és una matriu quadrada $n\times n$ i $B$ és una matriu també $n\times n$. Llavors $AB=\1_n$ si i només si $BA=\1_n$. En particular, si això passa, $A$ és invertible i la inversa és única. Denotem la matriu $B$ com $A^{-1}$.
\end{teorema}
\begin{proof}
	La primera part la demostrarem quan tinguem el concepte de rang d'una matriu (Observació \ref{obs:invuniq}). Vegem ara que si $B$ és tal que $AB=\1_n$, i $C$ tal que $CA=\1_n$, llavors $B=C$:
	$$
	C = C \1_n = C(AB)=(CA)B=\1_n B=B \,.
	$$
	Això implica que la inversa és única: Suposem $B'$ tal que $AB'=\1_n$, llavors, pel raonament d'abans, $B'=C=B$.
\end{proof}
\begin{proposicio}
	\begin{enumerate}[\rm (a)]
		\item Si $A \in M_{n\times n}(\K)$ és invertible, llavors $A^T$ també ho és i $(A^T)^{-1}=(A^{-1})^T$.
		\item Si $A,B \in M_{n\times n}(\K)$ són matrius invertibles, llavors el producte $AB$ també ho és i $(AB)^{-1}=B^{-1}A^{-1}$.
	\end{enumerate}
\end{proposicio}
\begin{proof}
	Demostrem primer (a): com que ens proposen una inversa, tant sols cal comprovar que ho és:
	$$
	(A^T)(A^{-1})^T=(A^{-1}A)^T=\1_n^T=\1_n \,.
	$$
	per tant, $(A^{-1})^T$ és inversa d'$A^T$ (com que $A^T$ és quadrada, pel Teorema \ref{teo:invuniq}, tant sols cal comprovar-ho per un dels costats).
	
	Demostrem ara (b): com que $A$ i $B$ són invertibles, existeixen les matrius inverses $A^{-1}$ i $B^{-1}\in M_{n\times n}(\K)$ respectivament. Llavors:
	$$
	(AB)(B^{-1}A^{-1})=A(BB^{-1})A^{-1}=A\1_n A^{-1}=A A^{-1}=\1_n
	$$
	Igual que (a), això ja demostra que $B^{-1}A^{-1}$ és inversa d'$AB$.
\end{proof}
\begin{observacio}
	D'aquí es dedueix que si $A_1, \dots, A_r \in M_{n\times n}(\K)$ són invertibles, el seu producte també ho és i $(A_1\cdots A_r)^{-1}=A_r^{-1}\cdots A_1^{-1}$.
\end{observacio}
\subsection{Transformacions elementals en matrius}\label{subsec:trans_el}\index{matriu!transformacions elementals}
Considerem una matriu organitzada per files (encara que tot el que farem aquí també es pot fer per columnes) i definim les \emph{transformacions elementals}:
\begin{center}
	\fbox{
		\begin{minipage}{0.95\textwidth}
			\begin{enumerate}[\bf T1.]
				\item Multiplicar una de les files per $\lambda\neq 0$.
				\item Sumar a una de les files $\mu$ vegades una altra fila.
				\item Intercanviar dues files.
			\end{enumerate}
		\end{minipage}
	}
\end{center}
%\begin{definicio}
%	Donada una matriu $A\in M_{m\times n}(K)$ fixada. Direm que $B$ és el resultat d'aplicar una transformació elemental a $A$ si els coeficients de $B$ són els mateixos que els de $A$ excepte:
%	\begin{enumerate}[\bf T1.]
%		\item Una de les files de $B$ té els corresponents valors de la mateixa fila d'$A$ multiplicats per $\lambda\neq 0$.
%		\item Una de les files de $B$ té els valors de la mateixa fila d'$A$ més $\mu$ vegades una altra fila d'$A$.
%		\item $B$ és el resultat d'intercanviar dues files d'$A$.
%	\end{enumerate}
%\end{definicio}
\begin{observacio}
	Si considerem la matriu d'un sistema d'equacions i apliquem qualsevol de les transformacions elementals, no modifiquem les solucions.
\end{observacio}
\begin{observacio}
	Les tres transformacions elementals es poden desfer mitjançant una transformació elemental:
	\begin{enumerate}[\bf 1.]
		\item Multiplicar una de les files per $1/\lambda\neq 0$.
		\item Sumar a una de les files $-\mu$ vegades una altra fila.
		\item Intercanviar dues files.
	\end{enumerate} 
\end{observacio}
Aquestes transformacions elementals es poden fer (i desfer) multiplicant per una matriu invertible $P$ per l'esquerra. Suposem que $A$ és una matriu amb $m$ files:
\begin{enumerate}[\bf T1.]
	\item si apliquem aquest canvi a la fila $i$ d'$\1_m$ tindrem una matriu $P$ on hem modificat el $1$ de la fila $i$ per $\lambda\neq 0$. Llavors $PA$ té els mateixos valors que $A$, però amb la fila $i$ multiplicada per $\lambda$. En aquest cas, $P^{-1}$ és una matriu identitat amb un $1/\lambda$ a la posició $(i,i)$.
	\item si sumem a la fila $i$ de la matriu $\1_n$ $\mu$ vegades la fila $k\neq i$ tindrem una matriu $P$ tal que $PA$ té a la fila $i$ la fila $i$ d'$A$ més $\mu$ vegades la fila $k$ d'$A$. En aquest cas, $P^{-1}$ és una matriu amb $1$ a la diagonal, $0$ fora, excepte la posició $(k,i)$, que val $-\mu$.
	\item si intercanviem les files $i$ i $k$ de la matriu $\1_n$ ($i\neq k$) obtenim una matriu $P$ tal que $PA$ és el resultat d'intercanviar les files $i$ i $k$ d'$A$. En aquest cas, $P^{-1}=P$.
\end{enumerate}
Amb aquests raonaments hem demostrat:
\begin{proposicio}
	Si considerem la matriu identitat $\1_m$ i li apliquem transformacions elementals per files, la matriu $P$ que obtenim és invertible. Si apliquem exactament les mateixes transformacions elementals a una altra matriu $A\in M_{m\times n}(\K)$, la matriu que resulta és exactament $PA$.
\end{proposicio}
\begin{definicio}\index{matriu!matrius equivalents}
	Diem que \emph{dues matrius $A$ i $B$ són equivalents per files} si es pot passar d'$A$ a $B$ mitjançant transformacions elementals per files. Escriurem $A \sim B$.
\end{definicio}
\begin{proposicio} \label{prop:relequiv} Si $A$, $B$ i $C$ són matrius de dimensions iguals, aleshores es té:
	\begin{itemize}
		\item $A \sim A$ (reflexiva),
		\item $A \sim B$ si i només si $B \sim A$ (simètrica),
		\item $A \sim B$ i $B \sim C$ implica $A \sim C$ (transitiva).
	\end{itemize}
\end{proposicio}
\begin{proof}
	La primera és \textbf{T2} per a qualsevol fila i $\mu=0$. 
	
	La segona és que la inversa d'una transformació elemental és una transformació elemental (i les composem en ordre invers): si $P_1$, \dots $P_r$ són les transformacions elementals que apliquem a $A$ per obtenir $B$, resulta que:
	$$
	P_r \cdots P_1 A=B
	$$
	llavors
	$$
	A= P_1^{-1} \cdots P_r^{-1} B
	$$
	Però si $P_i$ és una transformació elemental, $P_i^{-1}$ també i per tant $B\sim A$.
	
	La tercera és per definició de $\sim$, ja que passem de $A$ a $C$ fent primer els canvis elementals que transformen $A$ en $B$ i després els que transformen $B$ en $C$.
\end{proof}

\begin{observacio}
	Si tenim un conjunt $S$ i una relació $\sim$ entre els seus elements complint les propietats de la Proposició \ref{prop:relequiv} (reflexiva, simètrica i transitiva) diem que és una \emph{relació d'equivalència}\index{relació equivalència}.
\end{observacio}

\begin{definicio}
	\index{matriu!matriu esglaonada}
	Diem que una matriu $A$ està \emph{en forma esglaonada (per files)} si compleix que:
	\begin{enumerate}
		\item El primer element no nul de cada fila val $1$, i l'anomenem \emph{pivot}\index{matriu!pivot}.
		\item Si una fila conté un pivot a la columna $j$, les files superiors també tenen un pivot a una columna $j'<j$.
		\item Totes les entrades per sota d'un pivot valen $0$.
	\end{enumerate}
	
	\index{matriu!matriu redu{\"\i}da}
	Diem que una matriu $A$ està \emph{en forma reduïda (per files)} si està esglaonada i a més compleix que:
	\begin{enumerate}
		\item[3'] Si una columna té un pivot, aquest és l'únic element no nul de la seva columna.
	\end{enumerate}
\end{definicio}
\begin{exemple}
	De les matrius següents:
	\[
	A=\begin{pmatrix}
	1 & 2 & 0 \\ 0 & 0 & 1
	\end{pmatrix},
	B=\begin{pmatrix}
	1 & 0 & 1 \\ 0 & 0 & 1
	\end{pmatrix} \text{ i }
	C=\begin{pmatrix}
	0 & 1 & 0 \\ 1 & 0 & 1
	\end{pmatrix} ,
	D = \begin{pmatrix}
	1&2&3\\
	0&1&5
	\end{pmatrix}
	\]
	la matriu $A$ està reduïda per files, mentre que $B$ i $C$ no ho són. La matriu $D$ està esglaonada, però no reduïda.
\end{exemple}
Considerem ara l'algorisme següent (\textbf{Mètode de Gauss} o \textbf{Mètode de Gauss-Jordan})\index{mètode de Gauss}\index{mètode de Gauss-Jordan}, que aplica canvis elementals per files a una matriu $A$ fins que obtenim una matriu en forma reduïda per files. A aquest mètode també li diem \textbf{triangular la matriu per files}.
\begin{center}
	\fbox{
		\begin{minipage}{0.97\textwidth}
			Suposem que tenim una matriu $A\in M_{m\times n}(\K)$, on la submatriu formada per les $i_1$ primeres files ja té forma reduïda i si el pivot de la fila $i_1$ és a la posició $j_1$, els coeficients $a_{ij}=0$ si $i>i_1$ i $j<j_1$.
			
			\begin{enumerate}[\bf {G}1]
				\item Busquem la primera columna que no sigui tota zero a la submatriu formada per les files $i_1+1$ fins la $m$ (suposem columna $j_2$), i triem una fila a aquesta submatriu on hi hagi un element diferent de zero a aquesta columna. Si cal, apliquem el canvi \textbf{T3} per a que la fila $i_2:=i_1+1$ tingui un valor diferent de zero a aquesta columna. Podem suposar, doncs, que $a_{i_2j_2}\neq0$ i que $a_{ij}=0$ si $i>i_2$ i $j<j_2$.
				\item Volem que a la posició $(i_2,j_2)$ hi hagi un pivot ($=1$), per tant multipliquem tota la fila $i_2$ per $1/a_{i_2j_2}$ (canvi \textbf{T1}).
				\item Per a totes les files $i\neq i_2$ tals que $a_{ij_2}\neq 0$, hi restem la fila $i_2$ multiplicada per $a_{ij_2}$ (canvi \textbf{T2}), de tal manera que farem que a la posició $(i,j_2)$ hi hagi un zero per a tota $i\neq i_2$.
			\end{enumerate}
			Ara hem aconseguit que la submatriu formada per les files $1$, \dots ,$i_1+1$ sigui en forma reduïda.
			
			Iterem aquest procediment fins que $i_1=m$.
		\end{minipage}
	}
\end{center}
Com que la matriu té un nombre finit de files, aquest algorisme sempre acaba. A més, acabem de demostrar que:
\begin{teorema}
	Tota matriu $A\in M_{m\times n}(\K)$ és equivalent a una matriu reduïda.
\end{teorema}

\begin{exemple}
	Considerem la matriu:
	$$
	A=\begin{pmatrix}
	0 & 1 & 3 & 0 \\  1 & -2 & -5 & 4\\ 2 & -3 & -7 & 7
	\end{pmatrix}
	$$
	Apliquem el canvi \textbf{T3}, canviant la primera fila per la segona, obtenint:
	$$
	\begin{pmatrix}
	1 & -2 & -5 & 4\\0 & 1 & 3 & 0 \\   2 & -3 & -7 & 7
	\end{pmatrix}
	$$
	Apliquem el canvi \textbf{T2}, restant a la tercera fila $2$ cops la primera:
	$$
	\begin{pmatrix}
	1 & -2 & -5 & 4\\0 & 1 & 3 & 0 \\   0 & 1 & 3 & -1
	\end{pmatrix}
	$$
	Com que a la segona fila, la primera posició ja és $1$, podem utilitzar-la directament de pivot i sumar-la $2$ cops a la primera fila, i restar-la a la tercera:
	$$
	\begin{pmatrix}
	1 & 0 & 1 & 4\\0 & 1 & 3 & 0 \\   0 & 0 & 0 & -1
	\end{pmatrix}
	$$
	Les dues primeres files ja estan en forma reduïda, pel que podem considerar la tercera fila, i multiplicar-la per $-1$ per a que l'única posició no nu{\lgem}a sigui un pivot:
	$$
	\begin{pmatrix}
	1 & 0 & 1 & 4\\0 & 1 & 3 & 0 \\   0 & 0 & 0 & 1
	\end{pmatrix}
	$$
	Finalment restem la tercera fila a la primera multiplicada per $4$:
	$$
	\begin{pmatrix}
	1 & 0 & 1 & 0\\0 & 1 & 3 & 0 \\   0 & 0 & 0 & 1
	\end{pmatrix}
	$$
\end{exemple}

\subsection{Criteri d'invertibilitat. Rang d'una matriu}
Considerem primer el resultat següent:
\begin{teorema}\label{teo:critinv}\index{matriu!inversa}
	Donada una matriu quadrada $A \in M_{n\times n}(\K)$, les condicions següents són equivalents:
	\begin{enumerate}[\rm (a)]
		\item $A$ és equivalent a $\1_n$.
		\item $A$ és invertible.
	\end{enumerate}
\end{teorema}
\begin{proof}
	Vegem primer (a) implica (b): si $A$ és equivalent a $\1_n$, existeix $P$ tal que $PA=\1_n$. Com que $P$ és invertible, existeix $P^{-1}$ i podem fer:
	$$PA=\1_n \Rightarrow P ^{-1}PA=P^{-1} \Rightarrow A=P^{-1} \Rightarrow AP = P^{-1}P=\1_n$$
	per tant $P$ és la inversa d'$A$.
	
	Vegem (b) implica (a): suposem que $A$ no és equivalent a $\1_n$. Llavors, forçosament, quan l'esglaonem hi haurà una fila sense pivot, i per tant, tot zeros. Llavors obtenim $PA=A'$, amb $P$ invertible i $A'$ una matriu que té l'última fila tot zeros. Si $A$ fos invertible, voldria dir que existeix $Q$ tal que $AQ=\1_n$, llavors, però llavors:
	$$
	AQ=\1_n \Rightarrow PAQ=P \Rightarrow A'Q=P \Rightarrow A'(QP^{-1})=\1_n
	$$
	Observem ara que si $A'$ té l'última fila tot zeros $A'(QP^{-1})$ també, i per tant no pot ser $\1_n$.
\end{proof}
\begin{observacio}\label{obs:invuniq}
	Aquests resultats donen la demostració de la primera part del Teorema \ref{teo:invuniq}.
\end{observacio}
\begin{observacio}\label{obs:noinv}
	Si considerem $A \in M_{n\times n}(\K)$ que es pot subdividir en 4 submatrius:
	$$
	A=\left(\begin{array}{c|c}
	B & C \\ \hline \0 & D
	\end{array}\right)
	$$ 
	on la matriu $\0$ està formada per zeros i toca la diagonal (conté un coeficient amb coordenades $(i,i)$), llavors $A$ no és invertible.
\end{observacio}
\begin{proof}
	Primer observem que per a que una matriu sigui invertible, l'esglaonament ha de fer que a totes les seves columnes (i files) hi hagi un pivot (es dedueix del Teorema \ref{teo:critinv}).
	
	Si fos invertible podríem fer els canvis elementals per files i obtenir $\1_n$. Com que la submatriu $\0$ toca la diagonal, llavors $B$ té més columnes que files i $D$ més files que columnes. Quan esgraonem la matriu $A$, el pivot de les primeres columnes ha de ser a $B$ (sota hi ha zeros) i per tant no hi pot haver més pivots que files a $B$ (a les columnes de $B$), per tant hi ha columnes de $A$ sense pivot, i per tant no pot ser invertible.
\end{proof}

Podem utilitzar el concepte de matriu reduïda equivalent per a definir el rang d'una matriu $A$. Cal tenir en compte que necessitarem demostrar que la definició no depèn de quina matriu reduïda equivalent a $A$ considerem.

%Fem primer l'observació següent, que ens diu que les files diferent de zero d'una %matriu en forma reduïda són \emph{linealment independents}:
%\begin{observacio}\label{obs:linindep}\index{independència lineal}
%Si $A_1$, \dots , $A_{i_1}$ són les files diferents de zero d'una matriu $A$ reduïda %per files, i considerem $\lambda_1$, \dots , $\lambda_{i_1} \in \K$ tals que %$\lambda_1A_1+\cdots+\lambda_{i_1}A_{i_1}$ té tots els coeficients zero, llavors %$\lambda_1 = \cdots = \lambda_{i_1}=0$ (diem que les files $A_1$, \ldots, $A_{i_1}$ són %\emph{linealment independents}). 
%\end{observacio}

%\begin{proof}
%Com que $A$ està en forma reduïda, cada fila $i$ diferent de zero conté un pivot (=1), i d'aquí resulta que s'ha de complir $\lambda_i=0$. Això es pot aplicar a cada $i$.
%\end{proof}

\begin{proposicio}
	Si $A$ i $B$ són dues matrius en forma reduïda que són equivalents, llavors $A=B$. %tenen el mateix nombre de files diferent de zero.
\end{proposicio}
\begin{proof}
	Suposem que $A$ i $B$ són diferents. Considerem submatrius $A'$ i $B'$ formades per la primera columna on difereixin, així com per totes les columnes a l'esquerra d'aquesta que continguin pivots. Observem que $A'$ i $B'$ segueixen essent equivalents, mitjançant les mateixes transformacions elementals associades a $A$ i $B$. Podem interpretar $A'$ i $B'$ com matrius augmentades de sistemes lineals equivalents, posem en $r$ incògnites. Si aquests són compatibles, aleshores necessàriament els termes independents han de coincidir i, per tant obtenim una contradicció. Si aquests dos sistemes són incompatibles, això significa que l'última columna conté zeros en les primeres files (tantes com pivots) i necessàriament tindrem que aquesta última columna tindrà un pivot a l'entrada $r$-èssima. Per tant, obtenim una nova contradicció.
\end{proof}
%\begin{proof}
%	Vegem primer que tenen el mateix nombre de files diferent de zero: suposem que no, 
%que una de les matrius té més files diferents de zero que l'altra. Podem suposar que 
%$A$ té $i_1$ files diferent de zero, que $B$ té $i_2$ files diferent de zero i que 
%$i_1>i_2$.
%
%A més, com que les dues matrius són equivalents, existeix una matriu invertible $P$ (la
%composició de les transformacions elementals) tal que $PA=B$.
%
%Quan fem $PA=B$, estem dient que les files de $B$ són combinacions lineals de les d'$A$ %(que denotem per $A_1$, \dots $A_m$). Tenim que:
%$$
%0=B_{i_2+1}=p_{i_2+1,1}A_1+p_{i_2+1,2}A_2 + \cdots p_{i_2+1,i_1}A_{i_1}
%$$
%i per l'Observació \ref{obs:linindep}, 
%$p_{i_2+1,1}=p_{i_2+1,2}=\cdots=p_{i_2+1,i_1}=0$. Podem fer el mateix raonament per 
%totes les files entre $i_2+1$ i $n$, per tant tenim que la matriu $P$ és de la forma de
%l'Observació \ref{obs:noinv}, i per tant no és invertible, arribant a contradicció.

%Com que la contradicció ver de suposar que $A$ i $B$ tenen diferent nombre de files 
%diferent de zero, vol dir que han de tenir el mateix nombre de files no nu{\lgem}es.

%Falta veure que les files no nu{\lgem}es són iguals: com que són equivalents, existeix una 
%matriu invertible $P$ tal que $PA=B$, per tant, les files de $B$ són combinació lineal %de les d'$A$ (i les d'$A$ també són combinació de les de $B$). Això vol dir, en %particular, que els pivots estan a les mateixes columnes (si no, posant les matrius una %sobre l'altre $\big(\begin{smallmatrix}A \\ B \end{smallmatrix})\big)$, l'esglaonament %tindria més files diferent de zero).

%Mirem la primera fila d'$A$ i $B$: la posició del pivot bé donada per la primera %columna no nu{\lgem}a, i, com que són equivalents, ha de ser la mateixa, per tant (només %escric com a combinació lineal de les files no nu{\lgem}es, i que per tant tenen un pivot):
%$$
%(b_{11},b_{12},\dots,b_{1n})=(a_{11},a_{12},\dots,a_{1n})+\lambda_2(a_{21},a_{22},\dots%, a_{2n})+\cdots+\lambda_{i_1}(a_{i_11},a_{i_12},\dots,a_{i_1n})
%$$
%Utilitzem ara que els pivots estan a les mateixes columnes, que per tant tenen el %corresponent $b_{ij=0}$ per a veure que $\lambda_2=\dots=\lambda_{i_1}=0$.

%Aquest argument es pot utilitzar a totes les files no nu{\lgem}es: totes tenen un pivot a %la mateixa posició i zero on hi ha el pivot de les altres files.
%\end{proof}
\begin{notacio}
	D'aquí és dedueix que donada una matriu $A\in M_{n\times n}(\K)$, existeix una sola matriu reduïda equivalent a $A$, i l'anomenem $\rref(A)$ (\emph{reduced row-echelon form}) \index{rref}.
\end{notacio}
Ara ja podem definir el rang d'una matriu:
\begin{definicio}\index{matriu!rang}
	Donada una matriu $A$, definim el \emph{rang d'$A$} com el nombre de files diferents de zero (igual al nombre de pivots) de $\rref(A)$.
\end{definicio}
\begin{corollari}
	Si fem transformacions elementals a una matriu $A\in M_{m\times n}(\K)$, el rang de la matriu resultant és el mateix. Això també ho podem enunciar dient que si $P\in M_{n\times n}(\K)$ és una matriu invertible i $A\in M_{m\times n}(\K)$, llavors $\Rang(A)=\Rang(PA)$.
\end{corollari}
\begin{corollari}
	Una matriu quadrada $A\in M_{n\times n}(\K)$ és invertible si i només si té rang $n$.
\end{corollari}
\begin{proof}
	Considerem el Teorema \ref{teo:critinv}, i per tant és invertible si i només si és equivalent a $\1_n$, per tant, si i només si té rang $n$.
\end{proof}
El Mètode de Gauss ens dóna una manera de calcular la inversa d'una matriu: suposem que ens donen una matriu $A \in M_{n\times n}(\K)$. Considerem la matriu formada per una matriu identitat $n\times n$ al costat de la matriu $A$:
$$
\left(\begin{array}{c|c}A & \1_n \end{array}\right) \in M_{n\times 2n}(\K)
$$
Si apliquem canvis per files a aquesta matriu (cada fila té $2n$ coeficients) obtindrem matrius que podem escriure com:
$$
\left(\begin{array}{c|c}A'& P \end{array}\right) \in M_{n\times 2n}(\K)
$$
amb $A',P\in M_{n\times n}(\K)$ i que compleixen que $PA=A'$.

Com a cas particular tenim que, si $A$ és invertible, podem arribar a la situació
\begin{equation}\label{eq:inversa}
\left(\begin{array}{c|c}\1_n & P \end{array}\right) \in M_{n\times 2n}(\K)
\end{equation}
i tindrem que $P=A^{-1}$.

Si la matriu $A$ no fos invertible, el Teorema~\ref{teo:critinv} ens diu que no seria possible arribar a la situació de l'Equació \eqref{eq:inversa}.
\begin{exemple}
	Considerem la matriu:
	$$
	A=\begin{pmatrix*}[r]
	1 & 2 & 6 \\ 0 & -1 & -8 \\ 5 & 6 & 0
	\end{pmatrix*}
	$$
	Escrivim la matriu amb una còpia de la matriu identitat a la dreta:
	$$
	\left(\begin{array}{rrr|rrr}
	1 & 2 & 6 & 1 & 0 & 0 \\
	0 & -1 & -8& 0 & 1 & 0 \\
	5 & 6 & 0 & 0 & 0 & 1
	\end{array}\right)
	$$
	Esglaonem la part matriu resultant segon el mètode de Gauss:
	$$
	\left(\begin{array}{rrr|rrr}
	1 & 2 & 6 & 1 & 0 & 0\\
	0 & -1 & -8 & 0 & 1 & 0\\
	0 & -4 & -30 & -5 & 0 & 1
	\end{array}\right)
	\rightsquigarrow
	\left(\begin{array}{rrr|rrr}
	1 & 2 & 6 & 1 & 0 & 0\\
	0 & 1 & 8 & 0 & -1 & 0\\
	0 & -4 & -30 & -5 & 0 & 1
	\end{array}\right)
	$$
	$$
	\left(\begin{array}{rrr|rrr}
	1 & 0 & -10  & 1 & 2 & 0\\
	0 &1 & 8 & 0 & -1 & 0\\
	0 & 0 & 2 & -5 & -4 & 1
	\end{array}\right)
	\rightsquigarrow
	\left(\begin{array}{rrr|rrr}
	1 & 0 & 6 & 1 & 0 & 0\\
	0 & 1 & 8 & 0 & -1 & 0\\
	0 & 0 & 1 & -5/2 & -2 & 1/2
	\end{array}\right)
	$$
	$$
	\rightsquigarrow\left(\begin{array}{rrr|rrr}
	1 & 0 & 0 & -24 & -18 & 5\\
	0 & 1 & 0 & 20 & 15 & -4\\
	0 & 0 & 1 & -5/2 & -2 & 1/2
	\end{array}\right)
	$$
	Per tant:
	$$
	A^{-1}=\begin{pmatrix*}[r]
	-24 & -18 & 5 \\
	20 & 15 & -4 \\
	-5/2 & -2 & 1/2
	\end{pmatrix*}
	$$
	
\end{exemple}
\begin{exercici}
	Tot el que hem fet per files té el seu anàleg per columnes. Una manera senzilla d'adaptar totes les definicions i resultats és dir que la transposada compleix la definició per files. Per exemple:
	\begin{quote}
		La matriu $A$ és \emph{en forma reduïda per columnes} si $A^T$ és en forma reduïda per files.
	\end{quote}
	\begin{enumerate}[(a)]
		\item Enuncieu l'anàleg per columnes de cada resultat que s'ha vist a aquest capítol per files.
		\item Demostreu que $\Rang(A)=\Rang(A^T)$.
	\end{enumerate}
\end{exercici}
\begin{exercici}
	Considerem $A\in M_{m\times n}(\K)$ i hi afegim una fila, obtenint $A'\in M_{(m+1)\times n}(\K)$. Demostreu $\Rang(A)=\Rang(A')$ si i només si la fila que hem afegit és combinació lineal de les files d'$A$.
\end{exercici}
\begin{exercici}
	Demostreu que el rang d'una matriu $A$ és el nombre màxim de files (o columnes) linealment independents que conté $A$.
\end{exercici}
\subsection{Resolució de sistemes d'equacions lineals}
Recordem la notació d'un sistema d'equacions:
\begin{align*}
a_{11}x_1+a_{12}x_2+ \cdots + a_{1n}x_n &= b_1 \\
a_{21}x_1+a_{22}x_2+ \cdots + a_{2n}x_n &= b_2 \\
&\vdots \\
a_{m1}x_1+a_{m2}x_2+ \cdots + a_{mn}x_n &= b_m
\end{align*}
Que també escrivim com $AX=B$, on
\[
A=
\begin{pmatrix*}[r]
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} 
\end{pmatrix*}\text{, }
X=
\begin{pmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{pmatrix} \text{ i }
B=
\begin{pmatrix}
b_1 \\ b_2 \\ \vdots \\ b_m
\end{pmatrix}
\]
Volem esbrinar si el sistema té solució o no, i, en cas de tenir-ne, saber quantes en té i calcular-les.

Una primera interpretació és considerar que tenim una solució $X$ i fer el càlcul següent:
\[
\begin{pmatrix}
b_1 \\ b_2 \\ \vdots \\ b_m
\end{pmatrix} = B = AX =
x_1 \begin{pmatrix}
a_{11} \\ a_{21} \\ \vdots \\ a_{m1}
\end{pmatrix} +
x_2 \begin{pmatrix}
a_{12} \\ a_{22} \\ \vdots \\ a_{m2}
\end{pmatrix} + \cdots +
x_n \begin{pmatrix}
a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn}
\end{pmatrix}
\]
Per tant, una solució ens dóna $B$ com a combinació lineal de les columnes d'$A$, i al revés: si podem escriure $B$ com a combinació lineal de les columnes d'$A$, tenim una solució.

Argumentant amb el rang per columnes, ja tenim un criteri per saber si un sistema d'equacions lineals té solució o no:
\begin{proposicio}\index{sistema d'equacions!existència de solució}
	El sistema d'equacions:
	\begin{align*}
	a_{11}x_1+a_{12}x_2+ \cdots + a_{1n}x_n &= b_1 \\
	a_{21}x_1+a_{22}x_2+ \cdots + a_{2n}x_n &= b_2 \\
	&\vdots \\
	a_{m1}x_1+a_{m2}x_2+ \cdots + a_{mn}x_n &= b_m
	\end{align*}
	té solució si i només si:
	\[\Rang \begin{pmatrix*}[r]
	a_{11} & a_{12} & \cdots & a_{1n} \\
	a_{21} & a_{22} & \cdots & a_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{m1} & a_{m2} & \cdots & a_{mn} 
	\end{pmatrix*} =
	\Rang \begin{pmatrix*}[r]
	a_{11} & a_{12} & \cdots & a_{1n} & b_1 \\
	a_{21} & a_{22} & \cdots & a_{2n} & b_2 \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	a_{m1} & a_{m2} & \cdots & a_{mn} & b_m 
	\end{pmatrix*}
	\]
\end{proposicio}
\begin{proof}
	Denotem per $A$ la matriu associada al sistema i per $\overline{A}$ la matriu ampliada.
	Esglaonem per files la matriu $\overline{A}$ fins a tenir una matriu reduïda $P\overline{A}$ i fem exactament les mateixes transformacions a la matriu $A$, obtenint $PA$.
	Com que les primeres $n$ columnes d'$\overline{A}$ són precisament les d'$A$, tenim que $PA$ també és una matriu en forma reduïda, i tants sols difereixen de que $P\overline{A}$ té una columna més. 
	
	Si $\Rang(A)=\Rang(\overline{A})$, vol dir que a l'última columna de $P\overline{A}$ no hi ha cap pivot, i per tant es pot escriure l'última columna de $P\overline{A}$ com a combinació lineal de les $n$ primeres, obtenint una solució del sistema.
	
	Si $\Rang(A)\neq\Rang(\overline{A})$, per força $\Rang(\overline{A})=\Rang(A)+1$ i l'última columna té un pivot. La fila on hi ha el pivot de l'última columna correspon a l'equació $0=1$, que no té solució.
\end{proof}
El resultat anterior també es pot enunciar dient que si esglaonem per files la matriu ampliada del sistema, no queda cap fila on l'únic element no nul sigui a la columna de termes independents.

Com que les transformacions elementals per files a la matriu ampliada no modifiquen les solucions del sistema, suposem que hem esglaonat la matriu inicial del sistema. Llavors, obtenint una matriu reduïda $\overline{A}'$:
\begin{enumerate}[(a)]
	\item El sistema té solució si i només si l'última columna d'$\overline{A}'$ no té cap pivot. Si el sistema té solució, diem que el \emph{sistema és compatible}, i quan no té solució, diem que és un \emph{sistema incompatible}.
	\item Suposem que el sistema té solució, llavors cada pivot ens permet aïllar la variable corresponent a la seva columna.
	\item Continuem suposant que el sistema té solució: les columnes (de la matriu sense ampliar) que no tenen pivot definiran el que anomenem \emph{paràmetres lliures}\index{sistema d'equacions!paràmetres lliures}. N'hi haurà tants com $k:=n-\Rang(A)$, on $n$ és el número de incògnites. En aquest cas direm que \emph{la solució té dimensió $k$}.\\
	En el cas particular que $k=0$ diem que té solució única i diem que és un \emph{sistema compatible determinat}.\\
	Si $k>0$ diem que és un \emph{sistema compatible indeterminat amb $k$ paràmetres lliures}.
\end{enumerate}
\begin{exemple}\label{exem:SCI}
	Considerem el sistema d'equacions:
	
	\begin{align*}
	x - y + 2z + 3t &= 21 \\
	-x+2y+z+5t &= 26\\
	3x+y-2z+t &= -9\\
	3x+2y+z+9t &= 38
	\end{align*}
	
	Considerem la matriu ampliada i esglaonem:
	\[
	\begin{amatrix}{4}
	1 & -1 & 2 & 3 & 21 \\
	-1 & 2 & 1 & 5 & 26 \\
	3 & 1 & -2 & 1 & -9\\
	3 & 2 & 1 & 9 & 38
	\end{amatrix}
	\rightsquigarrow
	\begin{amatrix}{4}
	1 & -1 & 2 & 3 & 21 \\
	0 & 1 & 3 & 8 & 47 \\
	0 & 4 & -8 & -8 & -72\\
	0 & 5 & -5 & 0 & -25
	\end{amatrix}
	\]
	\[
	\begin{amatrix}{4}
	1 & 0 & 5 & 11 & 68 \\
	0 & 1 & 3 & 8 & 47 \\
	0 & 0 & -20 & -40 & -260\\
	0 & 0 & -20 & -40 & -260
	\end{amatrix}\rightsquigarrow
	\begin{amatrix}{4}
	1 & 0 & 5 & 11 & 68 \\
	0 & 1 & 3 & 8 & 47 \\
	0 & 0 & -20 & -40 & -260\\
	0 & 0 & 0 & 0 & 0
	\end{amatrix}
	\]
	\[
	\begin{amatrix}{4}
	1 & 0 & 5 & 11 & 68 \\
	0 & 1 & 3 & 8 & 47 \\
	0 & 0 & 1 & 2 & 13\\
	0 & 0 & 0 &  0 & 0
	\end{amatrix}
	\rightsquigarrow
	\begin{amatrix}{4}
	1 & 0 & 0 & 1 & 3 \\
	0 & 1 & 0 & 2 & 8 \\
	0 & 0 & 1 & 2 & 13\\
	0 & 0 & 0 &  0 & 0
	\end{amatrix}
	\]
	Com que el rang de la matriu associada és 3 i el de l'ampliada també, el sistema és compatible. Com que tenim 4 incògnites, té 4-3=1 paràmetre lliures (per tant, sistema compatible indeterminat). Amb l'esglaonament que hem fet, la $t$ és el paràmetre lliure i podem escriure la solució:
	\[
	\begin{pmatrix}
	x \\ y \\ z
	\end{pmatrix} =
	\begin{pmatrix}
	3 \\ 8 \\ 13
	\end{pmatrix}
	-t
	\begin{pmatrix}
	1 \\ 2 \\ 2
	\end{pmatrix} \text{ amb $t\in\K$.}
	\]
\end{exemple}
\begin{exemple}
	Per tal d'aprofitar els càlculs del sistema anterior, tant sols fem una petita modificació al l'últim terme independent:
	\begin{align*}
	x - y + 2z + 3t &= 21 \\
	-x+2y+z+5t&=26\\
	3x+y-2z+t&=-9\\
	3x+2y+z+9t&=39
	\end{align*}
	Considerem la matriu ampliada i esglaonem (són els mateixos passos d'abans, pel que tant sols escrivim la primera i última matriu):
	\[
	\begin{amatrix}{4}
	1 & -1 & 2 & 3 & 21 \\
	-1 & 2 & 1 & 5 & 26 \\
	3 & 1 & -2 & 1 & -9\\
	3 & 2 & 1 & 9 & 38
	\end{amatrix}
	\rightsquigarrow	
	\begin{amatrix}{4}
	1 & 0 & 0 & 1 & 3 \\
	0 & 1 & 0 & 2 & 8 \\
	0 & 0 & 1 & 2 & 13\\
	0 & 0 & 0 &  0 & 1
	\end{amatrix}
	\]
	si som estrictes en el concepte de reduïda, falta utilitzar l'$1$ de l'última fila per a posar zeros a l'última columna. En qualsevol cas, veiem que la matriu ampliada té rang 4, mentre que l'associada té rang 3, pel que el sistema és incompatible.
\end{exemple}

Considerem ara el cas particular en que $B=\0_m$ (un vector format per zeros).
\begin{definicio}
	\begin{enumerate}[(a)]
		\item Diem que el sistema d'equacions lineals $AX=B$ \emph{és homogeni}\index{sistema d'equacions!sistema homogeni} si $B=\0_m$.
		\item Si $AX=B$ és un sistema, parlem de \emph{sistema homogeni associat}\index{sistema d'equacions!sistema homogeni associat} al sistema $AX=\0_m$.
	\end{enumerate}
\end{definicio}
Tenim els resultats següents:
\begin{enumerate}[(a)]
	\item Si el sistema és homogeni, llavors $X=\0_n$ és una solució, per tant el sistema és compatible.
	\item Si $X$ és solució del sistema homogeni i $\lambda\in\K$, llavors $\lambda X$ també és solució del sistema: si ho pensem com a multiplicació de matrius, tenim la igualtat $A(\lambda X)=\lambda (AX)=\lambda \0_m=\0_m$, per tant $\lambda X$ també és solució.
	\item Si $X$ i $Y$ són solucions d'un sistema homogeni, llavors $X+Y$ també és solució: tornem a escriure-ho en forma matricial: $A(X+Y)=AX+AY=\0_m+\0_m=\0_m$.
	\item Si $AX=B$ és un sistema, amb $X$ i $Y$ una solucions, llavors $X-Y$ és una solució del sistema homogeni associat: $A(X-Y)=AX-AY=B-B=\0_m$.
	\item Si $AX=B$ és un sistema i $X$ una solució particular, qualsevol altre solució $Y$ es pot escriure com $Y=X+Z$, amb $Z$ solució del sistema homogeni associat: això es dedueix de l'apartat anterior: si $X$ i $Y$ són solucions, llavors $Z=Y-X$ és solució de l'homogeni. Si $X$ és solució del sistema i $Z$ de l'homogeni associat, llavors $AY=A(X+Z)=AX+AZ=B+\0_m=B$.
\end{enumerate}
\begin{exemple}
	Considerem el sistema d'equacions de l'Exemple \ref{exem:SCI}:
	\begin{align*}
	x + y + 2z + 3t &= 21 \\
	-x+2y+z+5t&=26\\
	3x+y-2z+t&=-9\\
	3x+2y+z+9t&=38
	\end{align*}
	Veiem que el sistema homogeni associat és
	\begin{align*}
	x + y + 2z + 3t &= 0\\
	-x+2y+z+5t&=0\\
	3x+y-2z+t&=0\\
	3x+2y+z+9t&=0
	\end{align*}
	i té per solució
	\[
	\begin{pmatrix}
	x \\ y \\ z
	\end{pmatrix} =
	t \begin{pmatrix}
	1 \\ 2 \\ 2
	\end{pmatrix} \text{ amb $t\in\K$.}
	\]
	Tenint en compte que una solució particular és $(x,y,z)=(3,8,13)$, podem escriure:
	\[
	\begin{pmatrix}
	x \\ y \\ z
	\end{pmatrix} =
	\begin{pmatrix}
	3 \\ 8 \\ 13
	\end{pmatrix}-
	t \begin{pmatrix}
	1 \\ 2 \\ 2
	\end{pmatrix} \text{ amb $t\in\K$.}
	\]
\end{exemple}
 
 \begin{llista-exercicis}
\item[Secció 1.1:] 6, 12, 16.
\item[Secció 1.2:] 10, 18.
\item[Secció 1.3:] 6, 8, 18, 20, 24, 28.
 \end{llista-exercicis}

